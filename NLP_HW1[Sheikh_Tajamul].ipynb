{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpK80XT3t8RL",
        "outputId": "d3e797e2-8d32-4878-9d64-60853ce39f42"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRIGRAM n=3\n",
        "import random\n",
        "def generate_trigram(cfd,first_words, length=50):\n",
        "    word1,word2 = first_words\n",
        "    print(word1,end=' ')\n",
        "    print(word2,end=' ')\n",
        "    for i in range(length):\n",
        "        pairs = cfd[(word1,word2)].most_common()\n",
        "        next_words = [word for word, freq in pairs]\n",
        "        freqs = [freq for word,freq in pairs]\n",
        "        word = random.choices(next_words, weights=freqs)[0]\n",
        "        word1=word2\n",
        "        word2=word\n",
        "        print(word,end=' ')\n",
        "trigrams = list(nltk.ngrams(text1,3))\n",
        "temp=[]\n",
        "for x in trigrams:\n",
        "    a1,a2,a3 = x\n",
        "    temp.append(((a1,a2),a3))\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(temp)\n",
        "generate_trigram(cfd,('this', 'world'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U12oTnPsYbt",
        "outputId": "768b2ed1-f1e8-4ae6-c5e7-db2bac06298d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this world charms from Gospel duty ! Woe to him !\" \" Oh good master , man ; but he didn ' t be flogged .' \"' Turn to ! Spring !\" Soon after , Queequeg disdained no seeming ignominy , if after he sounded , but a vacated thing , it "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram\n",
        "import random\n",
        "def generate_ngram(cfd,initial_words, length=50):\n",
        "    words = list(initial_words)\n",
        "    for x in initial_words:\n",
        "        print(x,end=' ')\n",
        "    for i in range(length):\n",
        "        key = tuple(words[-len(list(initial_words))+1:])\n",
        "        pairs = cfd[key].most_common()\n",
        "        next_words = [word for word, freq in pairs]\n",
        "        freqs = [freq for word,freq in pairs]\n",
        "        word = random.choices(next_words, weights=freqs)[0]\n",
        "        words.append(word)\n",
        "        print(word,end=' ')"
      ],
      "metadata": {
        "id": "q8ocWlBQs55S"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n=4\n",
        "ngrams = list(nltk.ngrams(text1, 4))\n",
        "temp = []\n",
        "for x in ngrams:\n",
        "    comp = x[:-1]\n",
        "    last_word = x[-1]\n",
        "    temp.append((comp, last_word))\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(temp)\n",
        "generate_ngram(cfd,('of','this','world', 'will'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIsMdehntZ2U",
        "outputId": "deecfef5-78af-4a3b-dd7c-ee782b422ae6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "of this world will ever warm ; and for much the same way as the whale swimming out from them , yet that hair - turbaned Fedallah remained a muffled mystery to the last degree of fatality ; those repeated disastrous repulses , all accumulating and piling their terrors upon Moby Dick ; those "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n=5\n",
        "ngrams = list(nltk.ngrams(text1, 5))\n",
        "temp = []\n",
        "for x in ngrams:\n",
        "    comp = x[:-1]\n",
        "    last_word = x[-1]\n",
        "    temp.append((comp, last_word))\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(temp)\n",
        "generate_ngram(cfd,('of','this', 'world','will','ever'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laaJmCtQtbiL",
        "outputId": "8c0cae82-3632-459b-f87b-aed74993088e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "of this world will ever warm ; and for whom even Pale Sherry would be too rosy - strong ; but with whom one sometimes loves to sit , and feel poor - devilish , too ; and grow convivial upon tears ; and say to themselves , so many hours hence this whale will "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nM0teVwQtdq3"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}